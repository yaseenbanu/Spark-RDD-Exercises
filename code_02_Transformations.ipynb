{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [mapPartitions(func)](#toc1_)    \n",
    "- [map(func)](#toc2_)    \n",
    "- [flatMap(func)](#toc3_)    \n",
    "- [filter(func)](#toc4_)    \n",
    "- [distinct([numPartitions])      ](#toc5_)    \n",
    "- [Set Operations](#toc6_)    \n",
    "  - [union(otherRDD)](#toc6_1_)    \n",
    "  - [intersection(otherRDD)](#toc6_2_)    \n",
    "  - [subtract(otherRDD)](#toc6_3_)    \n",
    "  - [cartesian(otherRDD)](#toc6_4_)    \n",
    "- [groupByKey-NumPartitions](#toc7_)    \n",
    "- [reduceByKey(func, [numPartitions])     ](#toc8_)    \n",
    "- [aggregateByKey(zeroValue)(seqFunc, combFunc, [numPartitions])     ](#toc9_)    \n",
    "- [sortByKey(ascending=True, numPartitions=None, keyfunc=<function>)](#toc10_)    \n",
    "- [join(otherRDD, [numPartitions])    ](#toc11_)    \n",
    "- [cogroup(otherRDD, [numPartitions])    ](#toc12_)    \n",
    "- [coalesce(numPartitions, shuffle=False)](#toc13_)    \n",
    "- [repartition(numPartitions)](#toc14_)    \n",
    "- [sample(withReplacement, fraction, seed=None)](#toc15_)    \n",
    "- [randomSplit(weights, seed=None)](#toc16_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD-Examples\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[mapPartitions(func)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mapPartitions` in Spark applies a function to each partition of an RDD or DataFrame. \n",
    "- It takes an iterable of rows as input and returns an iterable of processed rows. \n",
    "- This method is efficient for operations that benefit from accessing multiple rows at once, such as batch processing or adding new columns. \n",
    "- It reduces overhead by processing data in bulk rather than row-by-row, allowing for custom logic that leverages the entire partition's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Assuming 'auto_df' is your original DataFrame\n",
    "# For this example, let's create a sample DataFrame\n",
    "data = [(1970, 18.0), (1971, 15.0), (1972, 16.0)]\n",
    "columns = [\"modelyear\", \"mpg\"]\n",
    "sample_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the function to process each partition\n",
    "def add_processed_info(rows):\n",
    "    processed_rows = []\n",
    "    for row in rows:\n",
    "        # Process the row to create a new value\n",
    "        new_info = f\"Model Year: {row.modelyear}, MPG: {row.mpg}\"\n",
    "        # Append the new Row with the original data and the new info\n",
    "        processed_rows.append(Row(modelyear=row.modelyear, mpg=row.mpg, processed_info=new_info))\n",
    "    return processed_rows\n",
    "\n",
    "# Apply mapPartitions to add the new column\n",
    "new_rdd = sample_df.rdd.mapPartitions(add_processed_info)\n",
    "\n",
    "# Convert the RDD back to a DataFrame\n",
    "df_with_new_column = spark.createDataFrame(new_rdd)\n",
    "\n",
    "# Show the new DataFrame with the added column\n",
    "df_with_new_column.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD with 10 elements\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "\n",
    "# Define a function that takes an iterator of elements and returns the square of each element\n",
    "def square_partition(iter):\n",
    "    for x in iter:\n",
    "        yield x ** 2\n",
    "\n",
    "# Apply the function to each partition of the RDD using mapPartitions\n",
    "squared_rdd = rdd.mapPartitions(square_partition)\n",
    "\n",
    "# Print the resulting RDD\n",
    "print(squared_rdd.glom().collect())  # Prints [[1, 4, 9], [16, 25, 36], [49, 64, 81, 100]]\n",
    "\n",
    "print(squared_rdd.collect())  # Prints [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[map(func)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "result = rdd.map(lambda x: x * 2)\n",
    "print(result.collect())  # Output: [2, 4, 6, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd_1.map(lambda x: (x[0].upper(), x[1]))\n",
    "\n",
    "result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercease name: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[flatMap(func)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 4, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "result = rdd.flatMap(lambda x: (x, x*2))\n",
    "print(result.collect())  # Output: [1, 2, 2, 4, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9 = rdd5.flatMap(lambda x: x.split(\" \"))\n",
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[filter(func)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(result.collect())  # Output: [2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transformation: Filter records where age is greater than 30\n",
    "filtered_rdd = rdd_1.filter(lambda x: x[1] > 30)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[distinct([numPartitions])](#toc0_)       [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 3, 4])\n",
    "result = rdd.distinct()\n",
    "print(result.collect())  # Output: [1, 2, 3, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Set Operations](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_1: [7, 5, 1]\n",
      "rdd_2: [18, 16, 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_1=np.random.randint(0,10,3)\n",
    "rdd_1=sc.parallelize(list_1)\n",
    "\n",
    "list_2=np.random.randint(10,20,3)\n",
    "rdd_2=sc.parallelize(list_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[union(otherRDD)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([3, 4, 5])\n",
    "result = rdd1.union(rdd2)\n",
    "print(result.collect())  # Output: [1, 2, 3, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_1: [7, 5, 1]\n",
      "rdd_2: [18, 16, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 5, 1, 18, 16, 11]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"rdd_1:\",rdd_1.collect())\n",
    "print(\"rdd_2:\",rdd_2.collect())\n",
    "\n",
    "(rdd_1+rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[intersection(otherRDD)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([2, 3, 4])\n",
    "result = rdd1.intersection(rdd2)\n",
    "print(result.collect())  # Output: [2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_3_'></a>[subtract(otherRDD)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([2, 3, 4])\n",
    "result = rdd1.subtract(rdd2)\n",
    "print(result.collect())  # Output: [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_4_'></a>[cartesian(otherRDD)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 4), (2, 3), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([3, 4])\n",
    "result = rdd1.cartesian(rdd2)\n",
    "print(result.collect())  # Output: [(1, 3), (1, 4), (2, 3), (2, 4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_1: [7, 5, 1]\n",
      "rdd_2: [18, 16, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7, 18),\n",
       " (7, 16),\n",
       " (7, 11),\n",
       " (5, 18),\n",
       " (5, 16),\n",
       " (5, 11),\n",
       " (1, 18),\n",
       " (1, 16),\n",
       " (1, 11)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"rdd_1:\",rdd_1.collect())\n",
    "print(\"rdd_2:\",rdd_2.collect())\n",
    "\n",
    "rdd_1.cartesian(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[groupByKey-NumPartitions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', [2]), ('a', [1, 3])]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 3)])\n",
    "result = rdd.groupByKey().mapValues(list)\n",
    "print(result.collect())  # Output: [('a', [1, 3]), ('b', [2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of groupByKey\n",
    "data = [(1, 2), (2, 3), (1, 4), (2, 1),(1, 2), (2, 3), (1, 4), (2, 1)]\n",
    "rdd = sc.parallelize(data)\n",
    "result_data = rdd.groupByKey().collect()\n",
    "\n",
    "\n",
    "# Function to convert ResultIterable to list and print\n",
    "def print_result_iterable(result_iterable):\n",
    "    return list(result_iterable)\n",
    "\n",
    "# Print the contents of each iterable\n",
    "for key, iterable in result_data:\n",
    "    print(f\"Key: {key}, Iterable Contents: {print_result_iterable(iterable)}\")\n",
    "\n",
    "# Key: 1, Iterable Contents: [2, 4, 2, 4]\n",
    "# Key: 2, Iterable Contents: [3, 1, 3, 1]\n",
    "\n",
    "result_data\n",
    "\n",
    "# [(1, <pyspark.resultiterable.ResultIterable at 0x1e932d85a60>),\n",
    "#  (2, <pyspark.resultiterable.ResultIterable at 0x1e931c6d8b0>)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPairsRDD.groupByKey().mapValues(len).collect()\n",
    "\n",
    "# [('one', 1), ('two', 2), ('three', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPairsRDD.groupByKey().mapValues(list).collect()\n",
    "\n",
    "# [('one', [1]), ('two', [1, 1]), ('three', [1, 1, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[reduceByKey(func, [numPartitions])](#toc0_)      [&#8593;](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('a', 4)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 3)])\n",
    "result = rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(result.collect())  # Output: [('a', 4), ('b', 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceByKey transformation: Calculate the total age for each name\n",
    "reduced_rdd = rdd_1.reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of reduceByKey to sum values per key\n",
    "data = [(1, 2), (2, 3), (1, 4), (2, 1), (1, 2), (2, 3), (1, 4), (2, 1)]\n",
    "rdd = sc.parallelize(data)\n",
    "result = rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "result\n",
    "# Output:[(1, 12), (2, 8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ReduceByKey: Data are combined at each partition and only one output for each key at each paritiion to send \n",
    "over the network. It basically does everything ay the map side\n",
    "'''\n",
    "\n",
    "\n",
    "words = [\"one\", \"two\", \"two\", \"three\", \"three\", \"three\"]\n",
    "\n",
    "wordPairsRDD  =sc.parallelize(words).map(lambda a: (a,1))\n",
    "\n",
    "rdd1 = wordPairsRDD.reduceByKey(lambda a,b:a+b).collect() # [('one', 1), ('two', 2), ('three', 3)]\n",
    "rdd2 = wordPairsRDD.reduceByKey(add).collect() #[('one', 1), ('two', 2), ('three', 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[aggregateByKey(zeroValue)(seqFunc, combFunc, [numPartitions])](#toc0_)      [&#8593;](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('a', 4)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 3)])\n",
    "zero_value = 0\n",
    "seq_func = lambda acc, value: acc + value\n",
    "comb_func = lambda acc1, acc2: acc1 + acc2\n",
    "result = rdd.aggregateByKey(zero_value, seq_func, comb_func)\n",
    "print(result.collect())  # Output: [('a', 4), ('b', 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[sortByKey(ascending=True, numPartitions=None, keyfunc=<function>)](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('b', 2), ('a', 1), ('c', 3)])\n",
    "result = rdd.sortByKey()\n",
    "print(result.collect())  # Output: [('a', 1), ('b', 2), ('c', 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd_1.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[join(otherRDD, [numPartitions])](#toc0_)     [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 3)), ('b', (2, 4))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 2)])\n",
    "rdd2 = sc.parallelize([('a', 3), ('b', 4)])\n",
    "result = rdd1.join(rdd2)\n",
    "print(result.collect())  # Output: [('a', (1, 3)), ('b', (2, 4))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[cogroup(otherRDD, [numPartitions])](#toc0_)     [&#8593;](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ([1], [3, 4])), ('b', ([2], []))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 2)])\n",
    "rdd2 = sc.parallelize([('a', 3), ('a', 4)])\n",
    "result = rdd1.cogroup(rdd2).mapValues(lambda x: (list(x[0]), list(x[1])))\n",
    "print(result.collect())  # Output: [('a', ([1], [3, 4])), ('b', ([2], []))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc13_'></a>[coalesce(numPartitions, shuffle=False)](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
    "result = rdd.coalesce(2)\n",
    "print(result.getNumPartitions())  # Output: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc14_'></a>[repartition(numPartitions)](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 2)\n",
    "result = rdd.repartition(3)\n",
    "print(result.getNumPartitions())  # Output: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc15_'></a>[sample(withReplacement, fraction, seed=None)](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.sample(False, 0.4, 42)\n",
    "print(result.collect())  # Output: a random subset of elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc16_'></a>[randomSplit(weights, seed=None)](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "splits = rdd.randomSplit([0.5, 0.5], 42)\n",
    "print([split.collect() for split in splits])  # Output: two random subsets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
